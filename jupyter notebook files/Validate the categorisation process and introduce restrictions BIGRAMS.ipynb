{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-09T12:32:04.564152Z",
     "start_time": "2020-11-09T12:32:04.144819Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import pickle\n",
    "import datetime\n",
    "import telegram_send\n",
    "import collections\n",
    "from collections import Counter\n",
    "from multiprocessing import Pool, Lock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## for multicore machines, define how many cores should be used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nc = int(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## State if bigrams or trigrams are to be examined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ng = int(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the stemmed business model thesaurus and the bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## create a python dictionary containing the sets and descriptors for each set\n",
    "## in the business model thesaurus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-09T12:32:24.560633Z",
     "start_time": "2020-11-09T12:32:24.558315Z"
    }
   },
   "outputs": [],
   "source": [
    "path2bmt='/path/to/stemmed/business/model/thesaurus'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-09T12:32:26.327715Z",
     "start_time": "2020-11-09T12:32:26.296822Z"
    }
   },
   "outputs": [],
   "source": [
    "df=pd.read_excel(path2bmt,index_col=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-09T12:32:32.887767Z",
     "start_time": "2020-11-09T12:32:32.880642Z"
    }
   },
   "outputs": [],
   "source": [
    "sets=dict()\n",
    "for col in df:\n",
    "    sets[col]=df[col].dropna().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## create a list of the filepaths of the files containing the\n",
    "## text data scraped from the wayback machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-09T16:10:12.411003Z",
     "start_time": "2020-11-09T16:10:12.409079Z"
    }
   },
   "outputs": [],
   "source": [
    "if ng ==2:\n",
    "    path2wbm='/path/to/waybackmachine/bigrams'\n",
    "elif ng ==3:\n",
    "    path2wbm='/path/to/waybackmachine/trigrams'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-09T12:38:25.109278Z",
     "start_time": "2020-11-09T12:38:25.105859Z"
    }
   },
   "outputs": [],
   "source": [
    "dflist=[os.path.join(path2wbm,df) for df in os.listdir(path2wbm) if df.startswith('df')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the function for checking which terms in the text get tagged by the descriptors "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## here the function is defined that checks if the descriptors \n",
    "## appear in the text from the websites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-17T12:37:07.436498Z",
     "start_time": "2021-02-17T12:37:07.406395Z"
    }
   },
   "outputs": [],
   "source": [
    "def extract_keywords(sets, wordlist, new_keys=None, new_vals=None, vals_list=None,lst2=None):\n",
    "    \n",
    "    if new_keys is None:\n",
    "        new_keys=[]\n",
    "    if new_vals is None:\n",
    "        new_vals=[]\n",
    "    if vals_list is None:\n",
    "        vals_list=[]\n",
    "    if lst2 is None:\n",
    "        lst2=[]\n",
    "        \n",
    "    \n",
    "    for key, item in sets.items(): ## loop through the sets in the thesaurus\n",
    "        lst=[]\n",
    "        new_keys.append(key)\n",
    "        vals_lst2=[]\n",
    "        vals_list=[]\n",
    "        for val in item:   # loop through descriptors\n",
    "            vall=val.strip().split(' ')\n",
    "            \n",
    "            if not len(vall) == ng: # ensure only required n-grams are condsidered\n",
    "                continue\n",
    "\n",
    "            vals_list.append(val)\n",
    "            vals_set=set() # create a set that will include all terms from the website text that contain the descriptor\n",
    "            for wrd in wordlist: # loop through the list of terms in the text from website\n",
    "                wrdl=wrd.strip().split(' ')\n",
    "                \n",
    "                if not len(wrdl) == ng:\n",
    "                    continue\n",
    "                    \n",
    "                # set 1st specific restraint\n",
    "                if vall[1] == 'business':\n",
    "                    if vall[0] == wrdl[0] and vall[1] == wrdl[1]: # what to do if descriptor is contained within word from website text\n",
    "                        vals_set.add(wrd)\n",
    "                        if wrd not in lst:\n",
    "                            lst.append(wrd)\n",
    "                    else:\n",
    "                        continue\n",
    "                # 2nd specific restraint\n",
    "                elif vall[1] == 'handel':\n",
    "                    if not wrdl[1].endswith('handel'):\n",
    "                        continue\n",
    "                    else:\n",
    "                        if vall[0] in wrdl[0] and vall[1] in wrdl[1]: # what to do if descriptor is contained within word from website text\n",
    "                            vals_set.add(wrd)\n",
    "                            if wrd not in lst:\n",
    "                                lst.append(wrd)\n",
    "                # 3rd specific restraint\n",
    "                elif vall[0] == 'power':\n",
    "                    if not wrdl[0] == vall[0]:\n",
    "                        continue\n",
    "                    else:\n",
    "                        if vall[0] in wrdl[0] and vall[1] in wrdl[1]: # what to do if descriptor is contained within word from website text\n",
    "                            vals_set.add(wrd)\n",
    "                            if wrd not in lst:\n",
    "                                lst.append(wrd)\n",
    "                # 4th specific restraint\n",
    "                elif vall[0] == 'self':\n",
    "                    if not wrdl[0].endswith('self'):\n",
    "                        continue\n",
    "                    else:\n",
    "                        if vall[0] in wrdl[0] and vall[1] in wrdl[1]: # what to do if descriptor is contained within word from website text\n",
    "                            vals_set.add(wrd)\n",
    "                            if wrd not in lst:\n",
    "                                lst.append(wrd)\n",
    "                # 5th specific restraint\n",
    "                elif vall[0] == 'gruen':\n",
    "                    if 'gruend' in wrdl[0]:\n",
    "                        continue\n",
    "                    else:\n",
    "                        if vall[0] in wrdl[0] and vall[1] in wrdl[1]: # what to do if descriptor is contained within word from website text\n",
    "                            vals_set.add(wrd)\n",
    "                            if wrd not in lst:\n",
    "                                lst.append(wrd)\n",
    "                # 6th specific restraint\n",
    "                elif vall[0] == 'konventionel' and vall[1] == 'strom' or vall[1] == 'waerm':\n",
    "                    if 'erzeug' in wrdl[1]:\n",
    "                        continue\n",
    "                    else:\n",
    "                        if vall[0] in wrdl[0] and vall[1] in wrdl[1]: # what to do if descriptor is contained within word from website text\n",
    "                            vals_set.add(wrd)\n",
    "                            if wrd not in lst:\n",
    "                                lst.append(wrd)\n",
    "                        \n",
    "                else:\n",
    "                    if vall[0] in wrdl[0] and vall[1] in wrdl[1]: # what to do if descriptor is contained within word from website text\n",
    "                        vals_set.add(wrd)\n",
    "                        if wrd not in lst:\n",
    "                            lst.append(wrd)\n",
    "                        \n",
    "            vals_lst2.append(list(vals_set))\n",
    "        \n",
    "        new_words2=pd.DataFrame.from_dict(dict(zip(vals_list,vals_lst2)), orient='index').T\n",
    "        lst2.append(new_words2)\n",
    "        \n",
    "        new_vals.append(lst)\n",
    "    ## create a dataframe containing all of the terms from the text data\n",
    "    ## that has been tagged by descriptors\n",
    "    new_words=pd.DataFrame.from_dict(dict(zip(new_keys,new_vals)), orient='index').T\n",
    "    ## create a dictionary linking all of the decriptors\n",
    "    ## to the terms from the text data that they tagged\n",
    "    new_words_dict=dict(zip(new_keys,lst2))\n",
    "    \n",
    "    return new_words, new_words_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Excel writer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## prepare excel writer for exporting the dictionary of descriptors and\n",
    "## terms tagged by them to a\n",
    "## multi-spreadsheet excel file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-09T12:33:40.548102Z",
     "start_time": "2020-11-09T12:33:40.545487Z"
    }
   },
   "outputs": [],
   "source": [
    "from pandas import ExcelWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-09T12:33:40.710494Z",
     "start_time": "2020-11-09T12:33:40.707515Z"
    }
   },
   "outputs": [],
   "source": [
    "def save_xls(dict_df, path):\n",
    "    writer = ExcelWriter(path)\n",
    "    for key, item in dict_df.items():\n",
    "        item.to_excel(writer, key,index=False)\n",
    "        \n",
    "    writer.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-15T13:37:23.316352Z",
     "start_time": "2020-09-15T13:37:23.314125Z"
    }
   },
   "source": [
    "## Prepare temporary files to store the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-09T12:33:49.775162Z",
     "start_time": "2020-11-09T12:33:49.770956Z"
    }
   },
   "outputs": [],
   "source": [
    "outputpath='/path/to/output/folder'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-09T12:37:20.414821Z",
     "start_time": "2020-11-09T12:37:20.411361Z"
    }
   },
   "outputs": [],
   "source": [
    "keywords_tot=pd.DataFrame()\n",
    "keywords_dict_tot= collections.defaultdict(list)\n",
    "already_processed=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-09T12:37:20.909906Z",
     "start_time": "2020-11-09T12:37:20.907022Z"
    }
   },
   "outputs": [],
   "source": [
    "# create a temporary file to store results dataframe\n",
    "keywords_tot.to_pickle('kwt_temp.p')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-09T12:37:21.413893Z",
     "start_time": "2020-11-09T12:37:21.409486Z"
    }
   },
   "outputs": [],
   "source": [
    "# create a temporary file to store results dictionary\n",
    "with open('kwdt_temp.p','wb') as f:\n",
    "        pickle.dump(keywords_dict_tot,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-10T10:08:54.763262Z",
     "start_time": "2020-11-10T10:08:54.760908Z"
    }
   },
   "outputs": [],
   "source": [
    "# create a list of the files that have been already processed\n",
    "with open('already_processed_ngram.p','wb') as f:\n",
    "        pickle.dump(already_processed,f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-15T13:37:23.316352Z",
     "start_time": "2020-09-15T13:37:23.314125Z"
    }
   },
   "source": [
    "## Apply functions to all files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## here the function is defined that will be applied to each file containing the text from the websites\n",
    "## This will be applied utilising all available CPUs using the multiprocessing package "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-10T10:09:01.650256Z",
     "start_time": "2020-11-10T10:09:01.642638Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def extract(file):\n",
    "    \n",
    "    st=datetime.datetime.now()\n",
    "    fname=os.path.basename(file)\n",
    "    print(fname,datetime.datetime.now())\n",
    "    \n",
    "    with open('already_processed_ngram.p','rb') as f:\n",
    "        ap=pickle.load(f)\n",
    "    if fname in ap:\n",
    "        print(f'File {fname} already processed')\n",
    "        return\n",
    "    if fname == 'df10y197.p': # leave file 197 for later because it is very large and needs all the ram for itself\n",
    "        print(f'Found {fname}!!!')\n",
    "        return\n",
    "    \n",
    "    # load the file with the website text into a dataframe\n",
    "    df=pickle.load(open(file,'rb')) \n",
    "\n",
    "    cnt2=0\n",
    "    for col in df:\n",
    "        words=[]\n",
    "        for cell in df[col].dropna():\n",
    "            for wrd in cell.split(','):\n",
    "                words.append(wrd.strip())\n",
    "        \n",
    "        # apply the function defined in section 2 to the dataframe\n",
    "        keywords, keywords_dict=extract_keywords(sets,words)\n",
    "        \n",
    "        # update the temporary results dataframe file\n",
    "        lock.acquire()\n",
    "        keywords_tot=pd.read_pickle('kwt_temp.p')\n",
    "        keywords_tot=keywords_tot.append(keywords)\n",
    "        keywords_tot.to_pickle('kwt_temp.p')\n",
    "        lock.release()\n",
    "        \n",
    "        # update the temporary results dictionary file\n",
    "        lock.acquire()\n",
    "        with open('kwdt_temp.p','rb') as f:\n",
    "                keywords_dict_tot=pickle.load(f)\n",
    "\n",
    "        for key, items in keywords_dict.items():\n",
    "            if not key in keywords_dict_tot:\n",
    "                keywords_dict_tot[key]=items\n",
    "            else:\n",
    "                keywords_dict_tot[key]=pd.concat([keywords_dict_tot[key],items])\n",
    "            if cnt2 == 0:\n",
    "                print(f'dict_length = {len(keywords_dict_tot[key])}')\n",
    "            cnt2+=1\n",
    "        with open('kwdt_temp.p','wb') as f:\n",
    "            pickle.dump(keywords_dict_tot,f)\n",
    "        lock.release()\n",
    "    \n",
    "    # update the list of files that have been already processed\n",
    "    lock.acquire()\n",
    "    with open('already_processed_ngram.p','rb') as f:\n",
    "        ap=pickle.load(f)\n",
    "        ap.append(fname)\n",
    "        print(f'{len(ap)} files processed so far')\n",
    "    \n",
    "    with open('already_processed_ngram.p','wb') as f:\n",
    "        pickle.dump(ap,f)\n",
    "    lock.release()\n",
    "    \n",
    "    et=datetime.datetime.now()\n",
    "    print(f'Time taken for {fname} was {et-st}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## here the functions are defined for assigning the above function\n",
    "## to individual cores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-09T12:34:20.483245Z",
     "start_time": "2020-11-09T12:34:20.481185Z"
    }
   },
   "outputs": [],
   "source": [
    "def init(l):\n",
    "    global lock\n",
    "    lock = l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-09T12:34:21.470647Z",
     "start_time": "2020-11-09T12:34:21.467780Z"
    }
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "    l = Lock()\n",
    "    pool = Pool(initializer=init, initargs=(l,),processes=nc)\n",
    "    pool.map(extract, dflist)\n",
    "    pool.close()\n",
    "    pool.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-10T10:12:32.676153Z",
     "start_time": "2020-11-10T10:09:04.778274Z"
    }
   },
   "outputs": [],
   "source": [
    "starttime=datetime.datetime.now()\n",
    "main()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-12T11:54:19.961041Z",
     "start_time": "2020-11-12T11:54:19.957170Z"
    }
   },
   "outputs": [],
   "source": [
    "# file 197 is very large and needs to be processed individually due to memory restrictions\n",
    "ausnahme=[x for x in dflist if '197' in x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Working on file 197...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "st=datetime.datetime.now()\n",
    "file=ausnahme[0]\n",
    "fname=os.path.basename(file)\n",
    "print(fname,datetime.datetime.now())\n",
    "df=pickle.load(open(file,'rb')) # load the file with the website text into a dataframe\n",
    "words=[]\n",
    "for col in df:\n",
    "    words=[]\n",
    "    for cell in df[col].dropna():\n",
    "        for wrd in cell.split(','):\n",
    "            words.append(wrd.strip())\n",
    "\n",
    "    # apply the function defined in section 2 to the dataframe\n",
    "    keywords, keywords_dict=extract_keywords(sets,words)\n",
    "    \n",
    "    # update the temporary results dataframe file\n",
    "    keywords_tot=pd.read_pickle('kwt_temp.p')  # \n",
    "    keywords_tot=keywords_tot.append(keywords)\n",
    "    keywords_tot.to_pickle('kwt_temp.p')\n",
    "    \n",
    "    # update the temporary results dictionary file\n",
    "    with open('kwdt_temp.p','rb') as f:\n",
    "        keywords_dict_tot=pickle.load(f)\n",
    "        \n",
    "    for key, items in keywords_dict.items():\n",
    "        if not key in keywords_dict_tot:\n",
    "            keywords_dict_tot[key]=items\n",
    "        else:\n",
    "            keywords_dict_tot[key]=pd.concat([keywords_dict_tot[key],items])\n",
    "            \n",
    "    with open('kwdt_temp.p','wb') as f:\n",
    "        pickle.dump(keywords_dict_tot,f)\n",
    "        \n",
    "    # update the list of files that have been already processed\n",
    "    with open('already_processed_ngram.p','rb') as f:\n",
    "        ap=pickle.load(f)\n",
    "        ap.append(fname)\n",
    "    \n",
    "    with open('already_processed_ngram.p','wb') as f:\n",
    "        pickle.dump(ap,f)\n",
    "    \n",
    "et=datetime.datetime.now()\n",
    "print(f'Time taken for {fname} was {et-st}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "endtime=datetime.datetime.now()\n",
    "\n",
    "ttt=endtime-starttime\n",
    "\n",
    "print(f'Total time taken was {ttt}') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## define output filenames and save results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-10T10:41:02.961949Z",
     "start_time": "2020-11-10T10:41:02.959568Z"
    }
   },
   "outputs": [],
   "source": [
    "nw=datetime.datetime.now().strftime('%d_%m_%y@%H:%M')\n",
    "kwt_out=f'/home/ensys/anaconda3/Masterarbeit/Processed Data/kwt{ng}g_{nw}.xlsx'\n",
    "kwdt_out=f'/home/ensys/anaconda3/Masterarbeit/Processed Data/kwdt{ng}g_{nw}.xlsx'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-09T12:10:02.329260Z",
     "start_time": "2020-11-09T12:10:01.753640Z"
    }
   },
   "outputs": [],
   "source": [
    "keywords_tot=pd.read_pickle('kwt_temp.p')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-09T12:11:16.456318Z",
     "start_time": "2020-11-09T12:11:15.143949Z"
    }
   },
   "outputs": [],
   "source": [
    "ktf=keywords_tot.apply(lambda x: pd.Series(sorted(Counter(x.dropna()).items(), key=lambda pair: pair[1], reverse=True)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-09T12:19:34.451706Z",
     "start_time": "2020-11-09T12:19:24.589712Z"
    }
   },
   "outputs": [],
   "source": [
    "ktf.to_excel(kwt_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-09T12:22:05.542970Z",
     "start_time": "2020-11-09T12:22:03.868062Z"
    }
   },
   "outputs": [],
   "source": [
    "with open('kwdt_temp.p','rb') as f:\n",
    "            keywords_dict_tot=pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-09T12:22:33.709604Z",
     "start_time": "2020-11-09T12:22:33.706154Z"
    }
   },
   "outputs": [],
   "source": [
    "kwd=keywords_dict_tot.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-09T12:22:44.910591Z",
     "start_time": "2020-11-09T12:22:42.608280Z"
    }
   },
   "outputs": [],
   "source": [
    "for ky, df in kwd.items():\n",
    "    print(len(df),ky)\n",
    "    df=df.apply(lambda x: pd.Series(sorted(Counter(x.dropna()).items(), key=lambda pair: pair[1], reverse=True)))\n",
    "    kwd[ky]=df\n",
    "    print(len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-09T12:23:47.088199Z",
     "start_time": "2020-11-09T12:23:24.345347Z"
    }
   },
   "outputs": [],
   "source": [
    "save_xls(kwd,kwdt_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## send notification of successfull completion to telegram account"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-10T16:56:22.115258Z",
     "start_time": "2020-11-10T16:56:21.965327Z"
    }
   },
   "outputs": [],
   "source": [
    "telegram_send.send(messages=[f'Finished extracting {ng}-grams\\nFilename={nw} \\nTime taken = {ttt}'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
