{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import shutil\n",
    "import sys\n",
    "import hashlib\n",
    "from datetime import datetime as dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/path/to/waybackmachine/downloads'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputpath='/path/to/store/newly/organised/folders'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Organise wayback machine snapshots into folders corresponding to the month they are from"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def copy_files_monthly(folder, not_copied = None, not_kept = None):\n",
    "    if not_copied is None:\n",
    "        not_copied=[]\n",
    "    if not_kept is None:\n",
    "        not_kept=[]\n",
    "    fls=os.listdir(os.path.join(path,folder)) # list all files in folder\n",
    "    snaps=[item[:6] for item in fls if re.match('^\\d',item)]\n",
    "    snaps=list(dict.fromkeys(snaps))\n",
    "    snaps.sort()\n",
    "    for snap in snaps: # organise snapshots into folders corresponding to the month they are from\n",
    "        month=[item for item in os.listdir(os.path.join(path,folder)) if item.startswith(snap)]\n",
    "        base=os.path.basename(folder)\n",
    "        path_out=os.path.join(outputpath,base,snap)\n",
    "        for entry in month:\n",
    "            path_new=os.path.join(path,folder,entry)\n",
    "            for root, dirs, files in os.walk(path_new):\n",
    "                \n",
    "                if len(files)>0: # filter snapshots so that only relevant text data is retained\n",
    "                    files_tokeep=[item for item in files if '.htm' in item or '.php' in item or item.endswith('.html') or item.endswith('.htm')]\n",
    "                    files_tokeep2=[item for item in files_tokeep if item not in ['robots.txt', 'error.html']]\n",
    "                    not_keep=[item for item in files if item not in files_tokeep2]\n",
    "                    not_keep_txt=[item for item in not_keep if '.htm' in item or '.txt' in item]\n",
    "                    not_keep=[item.split('.')[-1] for item in not_keep]\n",
    "                    \n",
    "                    # make reference of what was not kept\n",
    "                    for pst in list(dict.fromkeys(not_keep)):\n",
    "                        not_copied.append(pst)\n",
    "                    for nk in not_keep_txt:\n",
    "                        not_kept.append(nk)\n",
    "                    \n",
    "                    # reassign files to be copied from old filepath to new filepath\n",
    "                    if len(files_tokeep2)>0:\n",
    "                        for file in files_tokeep2:\n",
    "                            old_name = os.path.join( os.path.abspath(root), file )\n",
    "                            base, extension = os.path.splitext(file)\n",
    "                            new_name = os.path.join(path_out, file)\n",
    "                            \n",
    "                            # check if output folder exists and if not then create it\n",
    "                            if not os.path.exists(path_out):\n",
    "                                os.makedirs(path_out)\n",
    "                            \n",
    "                            # copy file\n",
    "                            if not os.path.exists(new_name): \n",
    "                                shutil.copy(root+'/'+file,new_name)\n",
    "                            else:\n",
    "                                ii = 1\n",
    "                                \n",
    "                                while True:\n",
    "                                    new_name = os.path.join(path_out, base + \"_\" + str(ii) + extension)\n",
    "                                    if not os.path.exists(new_name):\n",
    "                                        shutil.copy(old_name, new_name)\n",
    "                                        break\n",
    "                                    ii += 1\n",
    "    print('File extensions not copied:',list(dict.fromkeys(not_copied)),\n",
    "          '\\n.Txt files not copied:',list(dict.fromkeys(not_kept)),'*',len(not_kept))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for folder in os.listdir(path):\n",
    "    copy_files_monthly(folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c=0\n",
    "for folder in os.listdir(path):\n",
    "    c+=1\n",
    "    print('_'*20)\n",
    "    print(c)\n",
    "    print(folder)\n",
    "    print('_'*20)\n",
    "    fldr=os.path.join(outputpath,folder)\n",
    "    if os.path.exists(fldr):\n",
    "        print('Folder has already been copied \\n')\n",
    "        continue\n",
    "    tm=dt.now().strftime('%m/%d %H:%M:%S')\n",
    "    print('Copying files from %s ....' % folder)\n",
    "    print('Copying started at %s \\n' % tm)\n",
    "    copy_files_monthly(folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-10T15:04:16.121419Z",
     "start_time": "2021-03-10T15:04:16.118784Z"
    }
   },
   "source": [
    "# Find and remove duplicate files in monthly folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## The following functions were found at:\n",
    "## https://stackoverflow.com/questions/748675/finding-duplicate-files-and-removing-them\n",
    "\n",
    "def chunk_reader(fobj, chunk_size=1024):\n",
    "    \"\"\"Generator that reads a file in chunks of bytes\"\"\"\n",
    "    while True:\n",
    "        chunk = fobj.read(chunk_size)\n",
    "        if not chunk:\n",
    "            return\n",
    "        yield chunk\n",
    "\n",
    "\n",
    "def get_hash(filename, first_chunk_only=False, hash=hashlib.sha1):\n",
    "    hashobj = hash()\n",
    "    file_object = open(filename, 'rb')\n",
    "\n",
    "    if first_chunk_only:\n",
    "        hashobj.update(file_object.read(1024))\n",
    "    else:\n",
    "        for chunk in chunk_reader(file_object):\n",
    "            hashobj.update(chunk)\n",
    "    hashed = hashobj.digest()\n",
    "\n",
    "    file_object.close()\n",
    "    return hashed\n",
    "\n",
    "\n",
    "def check_for_duplicates(paths, hash=hashlib.sha1, filenames = None, dups = None):\n",
    "    hashes_by_size = {}\n",
    "    hashes_on_1k = {}\n",
    "    hashes_full = {}\n",
    "    if filenames is None:\n",
    "        filenames=[]\n",
    "    if dups is None:\n",
    "        dups=[]\n",
    "\n",
    "    for path in paths:\n",
    "        for dirpath, dirnames, filenames in os.walk(path):\n",
    "            for filename in filenames:\n",
    "                full_path = os.path.join(dirpath, filename)\n",
    "                try:\n",
    "                    # if the target is a symlink (soft one), this will \n",
    "                    # dereference it - change the value to the actual target file\n",
    "                    full_path = os.path.realpath(full_path)\n",
    "                    file_size = os.path.getsize(full_path)\n",
    "                except (OSError,):\n",
    "                    # not accessible (permissions, etc) - pass on\n",
    "                    continue\n",
    "\n",
    "                duplicate = hashes_by_size.get(file_size)\n",
    "\n",
    "                if duplicate:\n",
    "                    hashes_by_size[file_size].append(full_path)\n",
    "                else:\n",
    "                    hashes_by_size[file_size] = []  # create the list for this file size\n",
    "                    hashes_by_size[file_size].append(full_path)\n",
    "\n",
    "    # For all files with the same file size, get their hash on the 1st 1024 bytes\n",
    "    for __, files in hashes_by_size.items():\n",
    "        if len(files) < 2:\n",
    "            continue    # this file size is unique, no need to spend cpy cycles on it\n",
    "\n",
    "        for filename in files:\n",
    "            try:\n",
    "                small_hash = get_hash(filename, first_chunk_only=True)\n",
    "            except (OSError,):\n",
    "                # the file access might've changed till the exec point got here \n",
    "                continue\n",
    "\n",
    "            duplicate = hashes_on_1k.get(small_hash)\n",
    "            if duplicate:\n",
    "                hashes_on_1k[small_hash].append(filename)\n",
    "            else:\n",
    "                hashes_on_1k[small_hash] = []          # create the list for this 1k hash\n",
    "                hashes_on_1k[small_hash].append(filename)\n",
    "\n",
    "    # For all files with the hash on the 1st 1024 bytes, get their hash on the full file - collisions will be duplicates\n",
    "    for __, files in hashes_on_1k.items():\n",
    "        if len(files) < 2:\n",
    "            continue    # this hash of fist 1k file bytes is unique, no need to spend cpy cycles on it\n",
    "\n",
    "        for filename in files:\n",
    "            try: \n",
    "                full_hash = get_hash(filename, first_chunk_only=False)\n",
    "            except (OSError,):\n",
    "                # the file access might've changed till the exec point got here \n",
    "                continue\n",
    "\n",
    "            duplicate = hashes_full.get(full_hash)\n",
    "            if duplicate:\n",
    "                #print(\"Duplicate found: %s and %s\" % (filename, duplicate))\n",
    "                filenames.append(filename)\n",
    "                dups.append(duplicate)\n",
    "            else:\n",
    "                hashes_full[full_hash] = filename\n",
    "    \n",
    "    return filenames,dups\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find and remove duplicate files\n",
    "for item in os.listdir(outputpath):\n",
    "    to_remove=[0]\n",
    "    duppath=os.path.join(outputpath,item)\n",
    "    print(duppath)\n",
    "\n",
    "    cnt=0\n",
    "    nof=0\n",
    "    while len(to_remove)>0:\n",
    "        cnt+=1\n",
    "        lists=check_for_duplicates([duppath])\n",
    "        orig_filenames=list(dict.fromkeys(lists[0]))\n",
    "        duplicates=list(dict.fromkeys(lists[1]))\n",
    "        to_remove=[item for item in duplicates if item not in orig_filenames]\n",
    "        nof=nof+len(to_remove)\n",
    "        print('Iteration:',cnt)\n",
    "        print('Total number of files removed:',nof)\n",
    "        for item in to_remove:\n",
    "            os.remove(item)\n",
    "            \n",
    "    # delete empty folders\n",
    "    for root, direc, file in os.walk(duppath):\n",
    "        for item in direc:\n",
    "            dirpath=os.path.join(root, item)\n",
    "            if len(os.listdir(dirpath))==0:\n",
    "                print(item)\n",
    "                os.rmdir(dirpath)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
