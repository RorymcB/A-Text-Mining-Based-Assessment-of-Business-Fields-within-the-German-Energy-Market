{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-08T16:03:06.019083Z",
     "start_time": "2021-02-08T16:03:05.528103Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import pickle\n",
    "import datetime\n",
    "import telegram_send\n",
    "from multiprocessing import Pool, Lock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-08T14:08:40.302963Z",
     "start_time": "2021-02-08T14:08:40.299344Z"
    }
   },
   "outputs": [],
   "source": [
    "## for multicore machines, define how many cores should be used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-08T14:08:40.884095Z",
     "start_time": "2021-02-08T14:08:40.881645Z"
    }
   },
   "outputs": [],
   "source": [
    "nc=int(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## State if bigrams or trigrams are to be examined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ng = int(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the stemmed business model thesaurus and the bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## create a python dictionary containing the sets and descriptors for each set\n",
    "## in the business model thesaurus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-31T15:29:48.175969Z",
     "start_time": "2021-01-31T15:29:48.172787Z"
    }
   },
   "outputs": [],
   "source": [
    "path2bmt='/path/to/stemmed/business/model/thesaurus'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-31T15:29:49.722432Z",
     "start_time": "2021-01-31T15:29:49.362780Z"
    }
   },
   "outputs": [],
   "source": [
    "df=pd.read_excel(path2bmt,index_col=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-31T15:29:50.643245Z",
     "start_time": "2021-01-31T15:29:50.620577Z"
    }
   },
   "outputs": [],
   "source": [
    "sets=dict()\n",
    "for col in df:\n",
    "    sets[col]=df[col].dropna().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## create a list of the filepaths of the files containing the\n",
    "## text data scraped from the wayback machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-12T11:54:06.494278Z",
     "start_time": "2020-11-12T11:54:06.492366Z"
    }
   },
   "outputs": [],
   "source": [
    "path2wbm='/path/to/waybackmachine/bigrams'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-12T11:54:07.068444Z",
     "start_time": "2020-11-12T11:54:07.061309Z"
    }
   },
   "outputs": [],
   "source": [
    "dflist=[os.path.join(path2wbm,df) for df in os.listdir(path2wbm) if df.startswith('df')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the function for tagging the terms in the text that fit the descriptors "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## here the function is defined that checks if the descriptors \n",
    "## appear in the text from the websites and then tags them if appropriate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-14T14:10:47.519045Z",
     "start_time": "2021-02-14T14:10:47.508033Z"
    }
   },
   "outputs": [],
   "source": [
    "def topic_distribution(cell):\n",
    "    dis=[]\n",
    "    if pd.isna(cell):\n",
    "        return None\n",
    "    for top in sets:  ## loop through the sets in the thesaurus\n",
    "        c=0\n",
    "        for val in sets[top]:   # loop through descriptors\n",
    "            vall=val.strip().split(' ')\n",
    "            \n",
    "            if not len(vall) == ng: # ensure only required n-grams are condsidered\n",
    "                continue\n",
    "                \n",
    "            for wrd in cell.split(','):\n",
    "                wrdl=wrd.strip().split(' ')\n",
    "                \n",
    "                if not len(wrdl) == ng:\n",
    "                    continue\n",
    "                \n",
    "                # set 1st specific restraint\n",
    "                if vall[1] == 'business':\n",
    "                    if vall[0] == wrdl[0] and vall[1] == wrdl[1]: # tag term if descriptor is contained within term from website text\n",
    "                        c+=1\n",
    "                    else:\n",
    "                        continue\n",
    "                # 2nd specific restraint\n",
    "                elif vall[1] == 'handel':\n",
    "                    if not wrdl[1].endswith('handel'):\n",
    "                        continue\n",
    "                    else:\n",
    "                        if vall[0] in wrdl[0] and vall[1] in wrdl[1]: # tag term if descriptor is contained within term from website text\n",
    "                            c+=1\n",
    "                        else:\n",
    "                            continue\n",
    "                # 3rd specific restraint\n",
    "                elif vall[0] == 'power':\n",
    "                    if not wrdl[0] == vall[0]:\n",
    "                        continue\n",
    "                    else:\n",
    "                        if vall[0] in wrdl[0] and vall[1] in wrdl[1]: # tag term if descriptor is contained within term from website text\n",
    "                            c+=1\n",
    "                        else:\n",
    "                            continue\n",
    "                # 4th specific restraint\n",
    "                elif vall[0] == 'self':\n",
    "                    if not wrdl[0].endswith('self'):\n",
    "                        continue\n",
    "                    else:\n",
    "                        if vall[0] in wrdl[0] and vall[1] in wrdl[1]: # tag term if descriptor is contained within term from website text\n",
    "                            c+=1\n",
    "                        else:\n",
    "                            continue\n",
    "                # 5th specific restraint\n",
    "                elif vall[0] == 'gruen':\n",
    "                    if 'gruend' in wrdl[0]:\n",
    "                        continue\n",
    "                    else:\n",
    "                        if vall[0] in wrdl[0] and vall[1] in wrdl[1]: # tag term if descriptor is contained within term from website text\n",
    "                            c+=1\n",
    "                        else:\n",
    "                            continue\n",
    "                # 6th specific restraint\n",
    "                elif vall[0] == 'konventionel' and vall[1] == 'strom' or vall[1] == 'waerm':\n",
    "                    if 'erzeug' in wrdl[1]:\n",
    "                        continue\n",
    "                    else:\n",
    "                        if vall[0] in wrdl[0] and vall[1] in wrdl[1]: # tag term if descriptor is contained within term from website text\n",
    "                            c+=1\n",
    "                        else:\n",
    "                            continue\n",
    "                            \n",
    "                else:\n",
    "                    if vall[0] in wrdl[0] and vall[1] in wrdl[1]: # tag term if descriptor is contained within term from website text\n",
    "                        c+=1\n",
    "                    else:\n",
    "                        continue\n",
    "                        \n",
    "        dis.append(c)\n",
    "\n",
    "        \n",
    "    return dis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare a new dataframe to contain the results of the categorisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-08T14:09:11.155685Z",
     "start_time": "2021-02-08T14:09:11.149691Z"
    }
   },
   "outputs": [],
   "source": [
    "cols=df.columns.tolist().copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-08T14:09:11.713107Z",
     "start_time": "2021-02-08T14:09:11.706948Z"
    }
   },
   "outputs": [],
   "source": [
    "cols.append('year')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-08T14:09:12.168858Z",
     "start_time": "2021-02-08T14:09:12.165169Z"
    }
   },
   "outputs": [],
   "source": [
    "cols.append('website')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-08T14:09:12.695443Z",
     "start_time": "2021-02-08T14:09:12.690271Z"
    }
   },
   "outputs": [],
   "source": [
    "new_df=pd.DataFrame(columns=cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare a list of the files that have been already processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-08T14:09:13.655099Z",
     "start_time": "2021-02-08T14:09:13.649301Z"
    }
   },
   "outputs": [],
   "source": [
    "already_processed=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-08T14:09:15.303379Z",
     "start_time": "2021-02-08T14:09:15.295496Z"
    }
   },
   "outputs": [],
   "source": [
    "with open('already_processed.p','wb') as f:\n",
    "        pickle.dump(already_processed,f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply functions to all files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## here the function is defined that will be applied to each file containing the text from the websites\n",
    "## This will be applied utilising all available CPUs using the multiprocessing package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-08T14:09:17.179261Z",
     "start_time": "2021-02-08T14:09:17.159988Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def top_dist(file):\n",
    "    st = datetime.datetime.now()\n",
    "    fname=os.path.basename(file)\n",
    "    print(fname,st)\n",
    "    \n",
    "    new_df=pd.DataFrame(columns=cols)\n",
    "\n",
    "    # load the file with the website text into a dataframe\n",
    "    df1 = pickle.load(open(file, 'rb'))\n",
    "    # apply the function defined in section 2 to the dataframe\n",
    "    dft = df1.applymap(topic_distribution)\n",
    "    for website in dft.index:\n",
    "        for col in dft.loc[website].dropna().index:\n",
    "            \n",
    "            dfn = pd.DataFrame.from_dict(dict(zip(cols, dft.loc[website, col])), orient='index').T\n",
    "            dfn['year'] = col\n",
    "            dfn['website'] = website\n",
    "            new_df = new_df.append(dfn)\n",
    "    \n",
    "    # update the temporary dataframe file with the results\n",
    "    lock.acquire()\n",
    "    df=pd.read_csv('test.csv')\n",
    "    df=df.append(new_df)\n",
    "    df.to_csv('test.csv',index=False)\n",
    "    lock.release()\n",
    "    \n",
    "    # update the list of files that have been already processed\n",
    "    lock.acquire()\n",
    "    with open('already_processed.p','rb') as f:\n",
    "        ap=pickle.load(f)\n",
    "        ap.append(fname)\n",
    "        print(f'{len(ap)} files processed so far')\n",
    "    \n",
    "    with open('already_processed.p','wb') as f:\n",
    "        pickle.dump(ap,f)\n",
    "    lock.release()\n",
    "    \n",
    "    endtime = datetime.datetime.now()\n",
    "    tt = endtime-st\n",
    "    print(f'Time taken for {fname} was {tt}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## here the functions are defined for assigning the above function\n",
    "## to individual cores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-08T14:09:18.329160Z",
     "start_time": "2021-02-08T14:09:18.318692Z"
    }
   },
   "outputs": [],
   "source": [
    "def init(l):\n",
    "    global lock\n",
    "    lock = l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-08T14:09:18.916666Z",
     "start_time": "2021-02-08T14:09:18.906717Z"
    }
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "    l = Lock()\n",
    "    pool = Pool(initializer=init, initargs=(l,),processes=nc)\n",
    "    pool.map(top_dist, dflist)\n",
    "    pool.close()\n",
    "    pool.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-08T14:09:21.353791Z",
     "start_time": "2021-02-08T14:09:21.349770Z"
    }
   },
   "outputs": [],
   "source": [
    "## export empty dataframe that will be used to store the results\n",
    "new_df.to_csv('test.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-08T14:11:24.961974Z",
     "start_time": "2021-02-08T14:10:25.422274Z"
    }
   },
   "outputs": [],
   "source": [
    "## apply functions to the files\n",
    "starttime=datetime.datetime.now()\n",
    "main()\n",
    "endtime=datetime.datetime.now()\n",
    "\n",
    "ttt=endtime-starttime\n",
    "print(f'Total time taken was {ttt}')   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-08T16:04:28.218744Z",
     "start_time": "2021-02-08T16:04:28.202978Z"
    }
   },
   "outputs": [],
   "source": [
    "df=pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-02T18:58:31.202255Z",
     "start_time": "2021-02-02T18:58:31.199892Z"
    }
   },
   "outputs": [],
   "source": [
    "nw=datetime.datetime.now().strftime('%d_%m_%y@%H:%M')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-23T20:09:41.867434Z",
     "start_time": "2020-10-23T20:09:35.363590Z"
    }
   },
   "outputs": [],
   "source": [
    "df.to_excel(f'/home/rory/wbm_downloads/topic_distribution/top_dist_2g_{nw}.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## send notification of successfull completion to telegram account"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-06T19:05:35.484395Z",
     "start_time": "2020-11-06T19:05:35.343408Z"
    }
   },
   "outputs": [],
   "source": [
    "telegram_send.send(messages=[f'Finished bigram categorisation\\nFilename={nw} \\nTotal time taken was {ttt}'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
