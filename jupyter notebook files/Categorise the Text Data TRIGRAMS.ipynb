{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-23T13:28:31.217682Z",
     "start_time": "2021-02-23T13:28:30.950769Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import pickle\n",
    "import datetime\n",
    "import telegram_send\n",
    "from multiprocessing import Pool, Lock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-23T13:28:31.567875Z",
     "start_time": "2021-02-23T13:28:31.566174Z"
    }
   },
   "outputs": [],
   "source": [
    "## for multicore machines, define how many cores should be used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-23T13:28:31.875538Z",
     "start_time": "2021-02-23T13:28:31.873221Z"
    }
   },
   "outputs": [],
   "source": [
    "nc=int(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-23T13:28:32.507813Z",
     "start_time": "2021-02-23T13:28:32.505725Z"
    }
   },
   "outputs": [],
   "source": [
    "## State if bigrams or trigrams are to be examined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-23T13:28:33.174426Z",
     "start_time": "2021-02-23T13:28:33.171791Z"
    }
   },
   "outputs": [],
   "source": [
    "ng = int(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the stemmed business model thesaurus and the trigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## create a python dictionary containing the sets and descriptors for each set\n",
    "## in the business model thesaurus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-31T15:29:48.175969Z",
     "start_time": "2021-01-31T15:29:48.172787Z"
    }
   },
   "outputs": [],
   "source": [
    "path2bmt='/path/to/stemmed/business/model/thesaurus'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-31T15:29:49.722432Z",
     "start_time": "2021-01-31T15:29:49.362780Z"
    }
   },
   "outputs": [],
   "source": [
    "df=pd.read_excel(path2bmt,index_col=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-31T15:29:50.643245Z",
     "start_time": "2021-01-31T15:29:50.620577Z"
    }
   },
   "outputs": [],
   "source": [
    "sets=dict()\n",
    "for col in df:\n",
    "    sets[col]=df[col].dropna().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## create a list of the filepaths of the files containing the\n",
    "## text data scraped from the wayback machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-12T11:54:06.494278Z",
     "start_time": "2020-11-12T11:54:06.492366Z"
    }
   },
   "outputs": [],
   "source": [
    "path2wbm='/path/to/waybackmachine/trigrams'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-12T11:54:07.068444Z",
     "start_time": "2020-11-12T11:54:07.061309Z"
    }
   },
   "outputs": [],
   "source": [
    "dflist=[os.path.join(path2wbm,df) for df in os.listdir(path2wbm) if df.startswith('df')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the function for tagging the terms in the text that fit the descriptors "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## here the function is defined that checks if the descriptors \n",
    "## appear in the text from the websites and then tags them if appropriate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-23T13:28:43.189074Z",
     "start_time": "2021-02-23T13:28:43.184797Z"
    }
   },
   "outputs": [],
   "source": [
    "def topic_distribution(cell):\n",
    "    dis=[]\n",
    "    if pd.isna(cell):\n",
    "        return None\n",
    "    for top in sets:  ## loop through the sets in the thesaurus\n",
    "        c=0\n",
    "        for val in sets[top]:   # loop through descriptors\n",
    "            vall=val.strip().split(' ')\n",
    "            \n",
    "            if not len(vall) == ng: # ensure only required n-grams are condsidered\n",
    "                continue\n",
    "                \n",
    "            for wrd in cell.split(','):\n",
    "                wrdl=wrd.strip().split(' ')\n",
    "                \n",
    "                if not len(wrdl) == ng:\n",
    "                    continue\n",
    "                \n",
    "                if vall[0] in wrdl[0] and vall[1] in wrdl[1] and vall[2] in wrdl[2]: # tag term if descriptor is contained within term from website text\n",
    "                    c+=1\n",
    "                else:\n",
    "                    continue\n",
    "                    \n",
    "        dis.append(c)\n",
    "\n",
    "        \n",
    "    return dis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare a new dataframe to contain the results of the categorisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-23T13:28:44.893143Z",
     "start_time": "2021-02-23T13:28:44.890797Z"
    }
   },
   "outputs": [],
   "source": [
    "cols=df.columns.tolist().copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-23T13:28:45.379664Z",
     "start_time": "2021-02-23T13:28:45.377369Z"
    }
   },
   "outputs": [],
   "source": [
    "cols.append('year')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-23T13:28:46.149731Z",
     "start_time": "2021-02-23T13:28:46.146386Z"
    }
   },
   "outputs": [],
   "source": [
    "cols.append('website')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-23T13:28:47.512848Z",
     "start_time": "2021-02-23T13:28:47.504350Z"
    }
   },
   "outputs": [],
   "source": [
    "new_df=pd.DataFrame(columns=cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare a list of the files that have been already processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-23T13:28:49.120630Z",
     "start_time": "2021-02-23T13:28:49.117214Z"
    }
   },
   "outputs": [],
   "source": [
    "already_processed=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-23T13:28:50.342912Z",
     "start_time": "2021-02-23T13:28:50.340633Z"
    }
   },
   "outputs": [],
   "source": [
    "with open('already_processed.p','wb') as f:\n",
    "        pickle.dump(already_processed,f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply functions to all files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## here the function is defined that will be applied to each file containing the text from the websites\n",
    "## This will be applied utilising all available CPUs using the multiprocessing package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-23T13:28:56.883306Z",
     "start_time": "2021-02-23T13:28:56.873229Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def top_dist(file):\n",
    "    st = datetime.datetime.now()\n",
    "    fname=os.path.basename(file)\n",
    "    print(fname,st)\n",
    "    \n",
    "    new_df=pd.DataFrame(columns=cols)\n",
    "\n",
    "    # load the file with the website text into a dataframe\n",
    "    df1 = pickle.load(open(file, 'rb'))\n",
    "    # apply the function defined in section 2 to the dataframe\n",
    "    dft = df1.applymap(topic_distribution)\n",
    "    for website in dft.index:\n",
    "        for col in dft.loc[website].dropna().index:\n",
    "            \n",
    "            dfn = pd.DataFrame.from_dict(dict(zip(cols, dft.loc[website, col])), orient='index').T\n",
    "            dfn['year'] = col\n",
    "            dfn['website'] = website\n",
    "            new_df = new_df.append(dfn)\n",
    "    \n",
    "    # update the temporary dataframe file with the results\n",
    "    lock.acquire()\n",
    "    df=pd.read_csv('test.csv')\n",
    "    df=df.append(new_df)\n",
    "    df.to_csv('test.csv',index=False)\n",
    "    lock.release()\n",
    "    \n",
    "    # update the list of files that have been already processed\n",
    "    lock.acquire()\n",
    "    with open('already_processed.p','rb') as f:\n",
    "        ap=pickle.load(f)\n",
    "        ap.append(fname)\n",
    "        print(f'{len(ap)} files processed so far')\n",
    "    \n",
    "    with open('already_processed.p','wb') as f:\n",
    "        pickle.dump(ap,f)\n",
    "    lock.release()\n",
    "    \n",
    "    endtime = datetime.datetime.now()\n",
    "    tt = endtime-st\n",
    "    print(f'Time taken for {fname} was {tt}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## here the functions are defined for assigning the above function\n",
    "## to individual cores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-23T13:28:57.843990Z",
     "start_time": "2021-02-23T13:28:57.839999Z"
    }
   },
   "outputs": [],
   "source": [
    "def init(l):\n",
    "    global lock\n",
    "    lock = l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-23T13:28:58.953248Z",
     "start_time": "2021-02-23T13:28:58.950480Z"
    }
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "    l = Lock()\n",
    "    pool = Pool(initializer=init, initargs=(l,),processes=nc)\n",
    "    pool.map(top_dist, dflist)\n",
    "    pool.close()\n",
    "    pool.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-23T13:29:01.064484Z",
     "start_time": "2021-02-23T13:29:01.059002Z"
    }
   },
   "outputs": [],
   "source": [
    "## export empty dataframe that will be used to store the results\n",
    "new_df.to_csv('test.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-23T19:06:56.090145Z",
     "start_time": "2021-02-23T13:29:03.666922Z"
    }
   },
   "outputs": [],
   "source": [
    "## apply functions to the files\n",
    "starttime=datetime.datetime.now()\n",
    "main()\n",
    "endtime=datetime.datetime.now()\n",
    "\n",
    "ttt=endtime-starttime\n",
    "print(f'Total time taken was {ttt}')   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-24T12:24:05.975689Z",
     "start_time": "2021-02-24T12:24:05.937465Z"
    }
   },
   "outputs": [],
   "source": [
    "df=pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-24T12:24:06.327479Z",
     "start_time": "2021-02-24T12:24:06.322546Z"
    }
   },
   "outputs": [],
   "source": [
    "nw=datetime.datetime.now().strftime('%d_%m_%y@%H:%M')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-24T12:24:26.011216Z",
     "start_time": "2021-02-24T12:24:20.029656Z"
    }
   },
   "outputs": [],
   "source": [
    "df.to_excel(f'/home/ensys/anaconda3/Masterarbeit/Processed Data/top_dist_3g_{nw}.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## send notification of successfull completion to telegram account"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-24T12:24:26.235838Z",
     "start_time": "2021-02-24T12:24:26.040336Z"
    }
   },
   "outputs": [],
   "source": [
    "telegram_send.send(messages=[f'Finished trigram categorisation\\nFilename={nw} \\nTotal time taken was {ttt}'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
