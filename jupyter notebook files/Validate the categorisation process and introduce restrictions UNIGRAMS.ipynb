{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-07T19:57:27.118585Z",
     "start_time": "2021-02-07T19:57:27.042629Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import pickle\n",
    "import datetime\n",
    "import telegram_send\n",
    "import collections\n",
    "from collections import Counter\n",
    "from multiprocessing import Pool, Lock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## for multicore machines, define how many cores should be used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nc = int(7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the stemmed business model thesaurus and the unigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## create a python dictionary containing the sets and descriptors for each set\n",
    "## in the business model thesaurus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-31T15:29:48.175969Z",
     "start_time": "2021-01-31T15:29:48.172787Z"
    }
   },
   "outputs": [],
   "source": [
    "path2bmt='/path/to/stemmed/business/model/thesaurus'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-31T15:29:49.722432Z",
     "start_time": "2021-01-31T15:29:49.362780Z"
    }
   },
   "outputs": [],
   "source": [
    "df=pd.read_excel(path2bmt,index_col=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-31T15:29:50.643245Z",
     "start_time": "2021-01-31T15:29:50.620577Z"
    }
   },
   "outputs": [],
   "source": [
    "sets=dict()\n",
    "for col in df:\n",
    "    sets[col]=df[col].dropna().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## create a list of the filepaths of the files containing the\n",
    "## text data scraped from the wayback machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-12T11:54:06.494278Z",
     "start_time": "2020-11-12T11:54:06.492366Z"
    }
   },
   "outputs": [],
   "source": [
    "path2wbm='/path/to/waybackmachine/unigrams'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-12T11:54:07.068444Z",
     "start_time": "2020-11-12T11:54:07.061309Z"
    }
   },
   "outputs": [],
   "source": [
    "dflist=[os.path.join(path2wbm,df) for df in os.listdir(path2wbm) if df.startswith('df')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the function for checking which terms in the text get tagged by the descriptors "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## here the function is defined that checks if the descriptors \n",
    "## appear in the text from the websites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-21T21:53:52.722975Z",
     "start_time": "2021-02-21T21:53:52.672356Z"
    }
   },
   "outputs": [],
   "source": [
    "def extract_keywords(sets, wordlist, new_keys=None, new_vals=None, vals_list=None,lst2=None):\n",
    "    \n",
    "    if new_keys is None:\n",
    "        new_keys=[]\n",
    "    if new_vals is None:\n",
    "        new_vals=[]\n",
    "    if vals_list is None:\n",
    "        vals_list=[]\n",
    "    if lst2 is None:\n",
    "        lst2=[]\n",
    "        \n",
    "    \n",
    "    for key, item in sets.items():\n",
    "        lst=[]\n",
    "        new_keys.append(key)\n",
    "        vals_lst2=[]\n",
    "        vals_list=[]\n",
    "        for val in item:   # loop through descriptors\n",
    "            if ' ' in val: # exclude bigrams and trigrams\n",
    "                continue\n",
    "            vals_list.append(val)\n",
    "            vals_set=set() # create a set that will include all terms from the website text that contain the descriptor\n",
    "            for wrd in wordlist: # loop through the list of terms in the text from website\n",
    "                \n",
    "                # set 1st specific restraint\n",
    "                if val == 'dashboard': \n",
    "                    if len(wrd) > 10:\n",
    "                        continue\n",
    "                    else:\n",
    "                        if val in wrd: # what to do if descriptor is contained within term from website text\n",
    "                            vals_set.add(wrd)\n",
    "                            if wrd not in lst:\n",
    "                                lst.append(wrd)\n",
    "                        else:\n",
    "                            continue\n",
    "                # 2nd specific restraint\n",
    "                elif val == 'plattform':\n",
    "                    if wrd.startswith('aussicht') or wrd.startswith('umspann') or \\\n",
    "                    wrd.startswith('oel') or wrd.startswith('konverter') or \\\n",
    "                    wrd.startswith('montage') or wrd.startswith('bohr'):\n",
    "                        continue\n",
    "                    else:\n",
    "                        if val in wrd: # what to do if descriptor is contained within term from website text\n",
    "                            vals_set.add(wrd)\n",
    "                            if wrd not in lst:\n",
    "                                lst.append(wrd)\n",
    "                        else:\n",
    "                            continue\n",
    "                # 3rd specific restraint\n",
    "                elif val == 'akku':\n",
    "                    if wrd.startswith('akkurat') or wrd.startswith('vakk') or wrd.startswith('akkustik'):\n",
    "                        continue\n",
    "                    else:\n",
    "                        if val in wrd: # what to do if descriptor is contained within term from website text\n",
    "                            vals_set.add(wrd)\n",
    "                            if wrd not in lst:\n",
    "                                lst.append(wrd)\n",
    "                        else:\n",
    "                            continue\n",
    "                # 4th specific restraint\n",
    "                elif val == 'efahrzeug' or val == 'emobilita' or val == 'wasserstoff':\n",
    "                    if not wrd.startswith(val):\n",
    "                        continue\n",
    "                    else:\n",
    "                        if val in wrd: # what to do if descriptor is contained within term from website text\n",
    "                            vals_set.add(wrd)\n",
    "                            if wrd not in lst:\n",
    "                                lst.append(wrd)\n",
    "                        else:\n",
    "                            continue\n",
    "                # 5th specific restraint\n",
    "                elif val == 'speicher':\n",
    "                    if not wrd.endswith('speicher'):\n",
    "                        continue\n",
    "                    elif wrd.startswith('arbeit') or wrd.startswith('daten'):\n",
    "                        continue\n",
    "                    else:\n",
    "                        if val in wrd: # what to do if descriptor is contained within term from website text\n",
    "                            vals_set.add(wrd)\n",
    "                            if wrd not in lst:\n",
    "                                lst.append(wrd)\n",
    "                        else:\n",
    "                            continue\n",
    "                # 6th specific restraint\n",
    "                elif key == 'Analytics' and val.startswith('analyse') or \\\n",
    "                key == 'Analytics' and val.startswith('daten'):\n",
    "                    if not wrd.startswith(val):\n",
    "                        continue\n",
    "                    else:\n",
    "                        if val in wrd: # what to do if descriptor is contained within term from website text\n",
    "                            vals_set.add(wrd)\n",
    "                            if wrd not in lst:\n",
    "                                lst.append(wrd)\n",
    "                        else:\n",
    "                            continue\n",
    "                            \n",
    "                # 7th specific restraint\n",
    "                elif key == 'Infrastructure Operation' and val.startswith('netz') or \\\n",
    "                key == 'Infrastructure Operation' and val.startswith('speicher') or \\\n",
    "                key == 'Infrastructure Operation' and val.startswith('leit') or \\\n",
    "                key == 'Infrastructure Operation' and val.startswith('schalt'):\n",
    "                    if not wrd.startswith(val):\n",
    "                        continue\n",
    "                    else:\n",
    "                        if val in wrd: # what to do if descriptor is contained within term from website text\n",
    "                            vals_set.add(wrd)\n",
    "                            if wrd not in lst:\n",
    "                                lst.append(wrd)\n",
    "                        else:\n",
    "                            continue\n",
    "                #8th specific restraint\n",
    "                elif key == 'Sector Coupling' and val.startswith('kopplung'):\n",
    "                    if not wrd.startswith(val):\n",
    "                        continue\n",
    "                    else:\n",
    "                        if val in wrd: # what to do if descriptor is contained within term from website text\n",
    "                            vals_set.add(wrd)\n",
    "                            if wrd not in lst:\n",
    "                                lst.append(wrd)\n",
    "                        else:\n",
    "                            continue\n",
    "                # 9th specific restraint\n",
    "                elif val == 'saas' or val == 'paas':\n",
    "                    if not wrd == val:\n",
    "                        continue\n",
    "                    else:\n",
    "                        if val in wrd: # what to do if descriptor is contained within term from website text\n",
    "                            vals_set.add(wrd)\n",
    "                            if wrd not in lst:\n",
    "                                lst.append(wrd)\n",
    "                        else:\n",
    "                            continue\n",
    "                # 10th specific restraint\n",
    "                elif key == 'Measuring' and val.startswith('eich') or \\\n",
    "                key == 'Measuring' and val.startswith('erst'):\n",
    "                    if not wrd.startswith(val):\n",
    "                        continue\n",
    "                    else:\n",
    "                        if val in wrd: # what to do if descriptor is contained within term from website text\n",
    "                            vals_set.add(wrd)\n",
    "                            if wrd not in lst:\n",
    "                                lst.append(wrd)\n",
    "                        else:\n",
    "                            continue\n",
    "                # 11th specific restraint\n",
    "                elif key == 'Engineering & Construction' and val.endswith('anlage'):\n",
    "                    if not wrd.endswith('anlage') or wrd.endswith('anlagen'):\n",
    "                        continue\n",
    "                    else:\n",
    "                        if val in wrd: # what to do if descriptor is contained within term from website text\n",
    "                            vals_set.add(wrd)\n",
    "                            if wrd not in lst:\n",
    "                                lst.append(wrd)\n",
    "                        else:\n",
    "                            continue\n",
    "                # 12th specific restraint\n",
    "                elif key == 'Conventional Production':\n",
    "                    if wrd.startswith('bio') or wrd.startswith('oeko') or wrd.startswith('natur') or wrd.startswith('blockheiz'):\n",
    "                        continue\n",
    "                    elif val.startswith('gas'):\n",
    "                        if wrd.startswith(val):\n",
    "                            if val in wrd: # what to do if descriptor is contained within term from website text\n",
    "                                vals_set.add(wrd)\n",
    "                                if wrd not in lst:\n",
    "                                    lst.append(wrd)\n",
    "                            else:\n",
    "                                continue\n",
    "                        else:\n",
    "                            continue\n",
    "                                \n",
    "                    elif val == 'kohle' or val == 'sprit' or val == 'torf' or val == 'uran':\n",
    "                        if not wrd == val:\n",
    "                            continue\n",
    "                        else:\n",
    "                            if val in wrd: # what to do if descriptor is contained within term from website text\n",
    "                                vals_set.add(wrd)\n",
    "                                if wrd not in lst:\n",
    "                                    lst.append(wrd)\n",
    "                            else:\n",
    "                                continue\n",
    "                    elif len(val) < 4: # if length of the descriptor is less than 4 then the length of the term from website text must be equal to the length of the descriptor\n",
    "                        if val == wrd: # what to do if descriptor is contained within term from website text\n",
    "                            vals_set.add(wrd)\n",
    "                            if wrd not in lst:\n",
    "                                lst.append(wrd)\n",
    "                        else:\n",
    "                            continue\n",
    "                                    \n",
    "                    else:\n",
    "                        if val in wrd: # what to do if descriptor is contained within term from website text\n",
    "                            vals_set.add(wrd)\n",
    "                            if wrd not in lst:\n",
    "                                lst.append(wrd)\n",
    "                        else:\n",
    "                            continue\n",
    "                \n",
    "                \n",
    "                # general restraint\n",
    "                elif len(val) < 4: # if length of the descriptor is less than 4 then the length of the term from website text must be equal to the length of the descriptor\n",
    "                    if val == wrd: # what to do if descriptor is contained within term from website text\n",
    "                        vals_set.add(wrd)\n",
    "                        if wrd not in lst:\n",
    "                            lst.append(wrd)\n",
    "                    else:\n",
    "                        continue\n",
    "                    \n",
    "                else:\n",
    "                    if val in wrd: # what to do if descriptor is contained within word from website text\n",
    "                        vals_set.add(wrd)\n",
    "                        if wrd not in lst:\n",
    "                            lst.append(wrd)\n",
    "                    else:\n",
    "                        continue\n",
    "            vals_lst2.append(list(vals_set))\n",
    "        \n",
    "        new_words2=pd.DataFrame.from_dict(dict(zip(vals_list,vals_lst2)), orient='index').T\n",
    "        lst2.append(new_words2)\n",
    "        \n",
    "        new_vals.append(lst)\n",
    "    ## create a dataframe containing all of the terms from the text data\n",
    "    ## that has been tagged by descriptors\n",
    "    new_words=pd.DataFrame.from_dict(dict(zip(new_keys,new_vals)), orient='index').T\n",
    "    ## create a dictionary linking all of the decriptors\n",
    "    ## to the terms from the text data that they tagged\n",
    "    new_words_dict=dict(zip(new_keys,lst2))\n",
    "    \n",
    "    return new_words, new_words_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Excel writer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## prepare excel writer for exporting the dictionary of descriptors and\n",
    "## terms tagged by them to a\n",
    "## multi-spreadsheet excel file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-07T19:56:47.790713Z",
     "start_time": "2021-02-07T19:56:47.601854Z"
    }
   },
   "outputs": [],
   "source": [
    "from pandas import ExcelWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-07T19:56:47.847795Z",
     "start_time": "2021-02-07T19:56:47.844274Z"
    }
   },
   "outputs": [],
   "source": [
    "def save_xls(dict_df, path):\n",
    "    writer = ExcelWriter(path)\n",
    "    for key, item in dict_df.items():\n",
    "        item.to_excel(writer, key,index=False)\n",
    "        \n",
    "    writer.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-15T13:37:23.316352Z",
     "start_time": "2020-09-15T13:37:23.314125Z"
    }
   },
   "source": [
    "## Apply functions to all files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-07T19:56:54.400835Z",
     "start_time": "2021-02-07T19:56:54.398551Z"
    }
   },
   "outputs": [],
   "source": [
    "outputpath='/path/to/output/folder'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-10T10:40:14.703821Z",
     "start_time": "2020-11-10T10:40:14.700290Z"
    }
   },
   "outputs": [],
   "source": [
    "keywords_tot=pd.DataFrame()\n",
    "keywords_dict_tot= collections.defaultdict(list)\n",
    "already_processed=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-10T10:40:18.712110Z",
     "start_time": "2020-11-10T10:40:18.708680Z"
    }
   },
   "outputs": [],
   "source": [
    "# create a temporary file to store results dataframe\n",
    "keywords_tot.to_pickle('kwt_temp.p')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-10T10:40:19.693268Z",
     "start_time": "2020-11-10T10:40:19.690792Z"
    }
   },
   "outputs": [],
   "source": [
    "# create a temporary file to store results dictionary\n",
    "with open('kwdt_temp.p','wb') as f:\n",
    "        pickle.dump(keywords_dict_tot,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-10T10:40:20.497139Z",
     "start_time": "2020-11-10T10:40:20.492252Z"
    }
   },
   "outputs": [],
   "source": [
    "# create a list of the files that have been already processed\n",
    "with open('already_processed_1gram.p','wb') as f:\n",
    "        pickle.dump(already_processed,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## here the function is defined that will be applied to each \n",
    "## file containing the text from the websites "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-10T10:40:33.306083Z",
     "start_time": "2020-11-10T10:40:33.298155Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def extract(file):\n",
    "    \n",
    "    st=datetime.datetime.now()\n",
    "    fname=os.path.basename(file)\n",
    "    print(fname,datetime.datetime.now())\n",
    "    \n",
    "    with open('already_processed_1gram.p','rb') as f:\n",
    "        ap=pickle.load(f)\n",
    "    if fname in ap:\n",
    "        print(f'File {fname} already processed')\n",
    "        return\n",
    "    if fname == 'df10y197.p': # leave file 197 for later because it is very large and needs all the ram for itself\n",
    "        print(f'Found {fname}!!!')\n",
    "        return\n",
    "    \n",
    "    \n",
    "    df=pickle.load(open(file,'rb')) # load the file with the website text into a dataframe\n",
    "    words=[]\n",
    "    for col in df:\n",
    "        words=[]\n",
    "        for cell in df[col].dropna():\n",
    "            for wrd in cell.split():\n",
    "                words.append(wrd.strip())\n",
    "        \n",
    "        # apply the function defined in section 2 to the dataframe\n",
    "        keywords, keywords_dict=extract_keywords(sets,words)\n",
    "        \n",
    "        # update the temporary results dataframe file\n",
    "        lock.acquire()\n",
    "        keywords_tot=pd.read_pickle('kwt_temp.p')\n",
    "        keywords_tot=keywords_tot.append(keywords)\n",
    "        keywords_tot.to_pickle('kwt_temp.p')\n",
    "        lock.release()\n",
    "\n",
    "        # update the temporary results dictionary file\n",
    "        lock.acquire()\n",
    "        with open('kwdt_temp.p','rb') as f:\n",
    "                keywords_dict_tot=pickle.load(f)\n",
    "\n",
    "        for key, items in keywords_dict.items():\n",
    "            if not key in keywords_dict_tot:\n",
    "                keywords_dict_tot[key]=items\n",
    "            else:\n",
    "                keywords_dict_tot[key]=pd.concat([keywords_dict_tot[key],items])\n",
    "                \n",
    "        with open('kwdt_temp.p','wb') as f:\n",
    "            pickle.dump(keywords_dict_tot,f)\n",
    "        lock.release()\n",
    "    \n",
    "    # update the list of files that have been already processed\n",
    "    lock.acquire()\n",
    "    with open('already_processed_1gram.p','rb') as f:\n",
    "        ap=pickle.load(f)\n",
    "        ap.append(fname)\n",
    "        print(f'{len(ap)} files processed so far')\n",
    "    \n",
    "    with open('already_processed_1gram.p','wb') as f:\n",
    "        pickle.dump(ap,f)\n",
    "    lock.release()\n",
    "    \n",
    "    et=datetime.datetime.now()\n",
    "    print(f'Time taken for {fname} was {et-st}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## here the functions are defined for assigning the above function\n",
    "## to individual cores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-10T10:41:25.853982Z",
     "start_time": "2020-11-10T10:41:25.850893Z"
    }
   },
   "outputs": [],
   "source": [
    "def init(l):\n",
    "    global lock\n",
    "    lock = l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-10T10:41:26.479140Z",
     "start_time": "2020-11-10T10:41:26.476543Z"
    }
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "    l = Lock()\n",
    "    pool = Pool(initializer=init, initargs=(l,),processes=nc)\n",
    "    pool.map(extract, dflist)\n",
    "    pool.close()\n",
    "    pool.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-10T16:56:21.825268Z",
     "start_time": "2020-11-10T10:41:29.482258Z"
    }
   },
   "outputs": [],
   "source": [
    "starttime=datetime.datetime.now()\n",
    "main()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-12T11:54:19.961041Z",
     "start_time": "2020-11-12T11:54:19.957170Z"
    }
   },
   "outputs": [],
   "source": [
    "# file 197 is very large and needs to be processed individually due to memory restrictions\n",
    "ausnahme=[x for x in dflist if '197' in x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Working on file 197...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file=ausnahme[0]\n",
    "fname=os.path.basename(file)\n",
    "print(fname,datetime.datetime.now())\n",
    "df=pickle.load(open(file,'rb')) # load the file with the website text into a dataframe\n",
    "words=[]\n",
    "for col in df:\n",
    "    words=[]\n",
    "    for cell in df[col].dropna():\n",
    "        for wrd in cell.split():\n",
    "            words.append(wrd.strip())\n",
    "\n",
    "            \n",
    "    # apply the function defined in section 2 to the dataframe\n",
    "    keywords, keywords_dict=extract_keywords(sets,words)\n",
    "    \n",
    "    # update the temporary results dataframe file\n",
    "    keywords_tot=pd.read_pickle('kwt_temp.p')  \n",
    "    keywords_tot=keywords_tot.append(keywords)\n",
    "    keywords_tot.to_pickle('kwt_temp.p')\n",
    "    \n",
    "    # update the temporary results dictionary file\n",
    "    with open('kwdt_temp.p','rb') as f:\n",
    "        keywords_dict_tot=pickle.load(f)\n",
    "        \n",
    "    for key, items in keywords_dict.items():\n",
    "        if not key in keywords_dict_tot:\n",
    "            keywords_dict_tot[key]=items\n",
    "        else:\n",
    "            keywords_dict_tot[key]=pd.concat([keywords_dict_tot[key],items])\n",
    "            \n",
    "    with open('kwdt_temp.p','wb') as f:\n",
    "        pickle.dump(keywords_dict_tot,f)\n",
    "    \n",
    "    # update the list of files that have been already processed\n",
    "    with open('already_processed_1gram.p','rb') as f:\n",
    "        ap=pickle.load(f)\n",
    "        ap.append(fname)\n",
    "    \n",
    "    with open('already_processed_1gram.p','wb') as f:\n",
    "        pickle.dump(ap,f)\n",
    "    \n",
    "et=datetime.datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-11T12:27:15.439155Z",
     "start_time": "2020-11-11T12:27:15.399913Z"
    }
   },
   "outputs": [],
   "source": [
    "endtime=datetime.datetime.now()\n",
    "\n",
    "ttt=endtime-starttime\n",
    "\n",
    "print(f'Total time taken was {ttt}')  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## define output filenames and save results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-07T19:57:36.235300Z",
     "start_time": "2021-02-07T19:57:36.232359Z"
    }
   },
   "outputs": [],
   "source": [
    "nw=datetime.datetime.now().strftime('%d_%m_%y@%H:%M')\n",
    "kwt_out=f'/home/ensys/anaconda3/Masterarbeit/Processed Data/kwt_{nw}.xlsx'\n",
    "kwdt_out=f'/home/ensys/anaconda3/Masterarbeit/Processed Data/kwdt_{nw}.xlsx'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-09T12:10:02.329260Z",
     "start_time": "2020-11-09T12:10:01.753640Z"
    }
   },
   "outputs": [],
   "source": [
    "keywords_tot=pd.read_pickle('kwt_temp.p')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-09T12:11:16.456318Z",
     "start_time": "2020-11-09T12:11:15.143949Z"
    }
   },
   "outputs": [],
   "source": [
    "ktf=keywords_tot.apply(lambda x: pd.Series(sorted(Counter(x.dropna()).items(), key=lambda pair: pair[1], reverse=True)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-09T12:19:34.451706Z",
     "start_time": "2020-11-09T12:19:24.589712Z"
    }
   },
   "outputs": [],
   "source": [
    "ktf.to_excel(kwt_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-07T20:13:34.131361Z",
     "start_time": "2021-02-07T20:13:31.264613Z"
    }
   },
   "outputs": [],
   "source": [
    "with open('kwdt_temp.p','rb') as f:\n",
    "            keywords_dict_tot=pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-07T20:13:36.129465Z",
     "start_time": "2021-02-07T20:13:36.074114Z"
    }
   },
   "outputs": [],
   "source": [
    "kwd=keywords_dict_tot.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-07T20:13:41.746053Z",
     "start_time": "2021-02-07T20:13:37.875987Z"
    }
   },
   "outputs": [],
   "source": [
    "for ky, df in kwd.items():\n",
    "    print(len(df),ky)\n",
    "    df=df.apply(lambda x: pd.Series(sorted(Counter(x.dropna()).items(), key=lambda pair: pair[1], reverse=True)))\n",
    "    kwd[ky]=df\n",
    "    print(len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-07T19:58:52.559468Z",
     "start_time": "2021-02-07T19:58:08.330817Z"
    }
   },
   "outputs": [],
   "source": [
    "save_xls(kwd,kwdt_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## send notification of successfull completion to telegram account"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-10T16:56:22.115258Z",
     "start_time": "2020-11-10T16:56:21.965327Z"
    }
   },
   "outputs": [],
   "source": [
    "telegram_send.send(messages=[f'Finished extracting unigrams\\nFilename={nw} \\nTime taken = {ttt}'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
